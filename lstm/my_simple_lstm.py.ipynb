{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(v): return 1/(1+np.exp(v))\n",
    "\n",
    "# returns forward-propagation output, hidden, candidate, and forget values\n",
    "# params should be object with 1 dim for each h_t, x_t, cin_t and\n",
    "#   for those values each parameters for the input, forget, candidate and output gates\n",
    "# candidate gate is special - uses tanh, others sigmoid\n",
    "# Then we update the candidate state and output the hidden values\n",
    "# c_t = f_t * c_t-1 + i_t * cin_t\n",
    "def forward(t):\n",
    "    # gates\n",
    "    inp[t] = sigmoid(np.dot(wxi, x[t]) + whi*h[t-1] + bi)\n",
    "    f[t] = sigmoid(np.dot(wxf, x[t]) + whf*h[t-1] + bf)\n",
    "    o[t] = sigmoid(np.dot(wxo, x[t]) + who*h[t-1] + bo)\n",
    "    # input transform\n",
    "    cin[t] = np.tanh(np.dot(wxcin, x[t]) + whcin*h[t-1] + bcin)\n",
    "    # state update\n",
    "    c[t] = cin[t]*f[t] + inp[t]*cin[t]\n",
    "    h[t] = o[t]*np.tanh(c[t])\n",
    "    \n",
    "# initiate i, f, o, cin, c, h for t steps\n",
    "# they should all be scalar values?\n",
    "\n",
    "# calculate for all timesteps\n",
    "# for t in range(1, T):\n",
    "#     forward(t)\n",
    "    \n",
    "def backward(t):\n",
    "    # calculate loss at some time t, whatever loss function is used\n",
    "    # calculate the derivative of the loss using h[t] as the predicted value and y[t] as true value\n",
    "    # this is dL/dh (dLdh)\n",
    "    dLdh = h[t] - y[t] # FIXME e.g. derivative of the loss function\n",
    "    dc = o[t]*dLdh\n",
    "    do = c[t]*dLdh\n",
    "    # gate output derivatives\n",
    "    dcin = inp[t]*dc\n",
    "    di = cin[t]*dc\n",
    "    df = c[t-1]*dc # not sure about this\n",
    "    # gate input derivatives\n",
    "    di_input = (1-inp[t])*inp[t]*di\n",
    "    df_input = (1-f[t])*f[t]*df\n",
    "    do_input = (1-o[t])*o[t]*do\n",
    "    dcin_input = (1-cin[t]**2)*dcin\n",
    "    # derivatives wrt inputs\n",
    "    wi_deriv += np.outer(di_input, xc) # xc = (x[t], h[t-1])\n",
    "    wf_deriv += np.outer(df_input, xc)\n",
    "    wo_deriv += np.outer(do_input, xc)\n",
    "    wcin_deriv += np.outer(dcin_input, xc)\n",
    "    \n",
    "# x will by a series of letters\n",
    "word = list(\"hello\")\n",
    "xchars = word[0:(len(word)-1)]\n",
    "ychars = word[1:len(word)]\n",
    "\n",
    "# represent a series of letters as 1 hot vectors\n",
    "char_to_idx = {}\n",
    "idx_to_char = {}\n",
    "for idx, char in enumerate(list(set(word))):\n",
    "    char_to_idx[char] = idx\n",
    "    idx_to_char[idx] = char\n",
    "\n",
    "def word_to_one_hot_matrix(char_arr):\n",
    "    word_matrix = []\n",
    "    for char in char_arr:\n",
    "        char_arr = [0]*len(char_to_idx)\n",
    "        char_idx = char_to_idx[char]\n",
    "        char_arr[char_idx] = 1\n",
    "        word_matrix.append(char_arr)\n",
    "    return word_matrix\n",
    "\n",
    "x = word_to_one_hot_matrix(xchars)\n",
    "y = word_to_one_hot_matrix(ychars)\n",
    "\n",
    "def random_params():\n",
    "    return [np.random.rand() for i in range(len(xchars))]\n",
    "\n",
    "def random_bias(): return np.random.rand()\n",
    "\n",
    "def zero_vector(): return [0]*len(xchars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.25058273  0.25353739  0.23634927  0.25953061]\n",
      "1.44244459591\n",
      "[ 0.25058273  0.25353739 -0.76365073  0.25953061]\n"
     ]
    }
   ],
   "source": [
    "wxi, wxf, wxo, wxcin, why = [random_params() for i in range(5)]\n",
    "bi, bf, bo, bcin, whi, whf, who, whcin = [random_bias() for i in range(8)]\n",
    "inp, f, o, cin, c, h =  [zero_vector() for i in range(6)]\n",
    "t = 1\n",
    "forward(t)\n",
    "\n",
    "# predictions for next letter\n",
    "out_score = np.dot(h[t], why)\n",
    "y_preds = np.exp(out_score)/np.sum(np.exp(out_score))\n",
    "print y_preds\n",
    "# FIXME - but this should be sigmoid?\n",
    "\n",
    "# loss is true y[t] - y_pred (but should actually be cross entropy,\n",
    "# although this may be the true derivative of the cross entropy loss?)\n",
    "yidx = int(np.where(np.array(y[t]) == 1)[0])\n",
    "loss = -np.log(y_preds[yidx])\n",
    "print loss\n",
    "df = y_preds - y[t]\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08242533  0.01134818  2.32191714  0.19794776]\n",
      "1.20137988934\n"
     ]
    }
   ],
   "source": [
    "dW = h[t]*df\n",
    "reg = 1e-3 # regularization strength\n",
    "step_size = 1e-0\n",
    "dW += [why[i]*reg for i in range(len(why))]\n",
    "why += -step_size * dW\n",
    "print why\n",
    "\n",
    "new_out_score = np.dot(h[t], why)\n",
    "new_y_preds = np.exp(new_out_score)/np.sum(np.exp(new_out_score))\n",
    "loss = -np.log(new_y_preds[yidx])\n",
    "print loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
