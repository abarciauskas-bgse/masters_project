{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2401"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "execfile('midi_to_state.py')\n",
    "daft_state = midiToNoteStateMatrix('daft_punk-one_more_time.mid')\n",
    "len(daft_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381\n",
      "381\n"
     ]
    }
   ],
   "source": [
    "def encode_midi_states(state_mat):\n",
    "    chord_to_idx = {}\n",
    "    idx_to_chord = {}\n",
    "    for sixteenth_note in state_mat:\n",
    "        notes_only = [note[0] for note in sixteenth_note]\n",
    "        note_idcs_set = tuple([i for i, x in enumerate(notes_only) if x == 1])\n",
    "        if len(note_idcs_set) == 0: note_idcs_set = (None)\n",
    "        if not note_idcs_set in chord_to_idx.keys():\n",
    "            new_idx = len(chord_to_idx) + 1\n",
    "            chord_to_idx[note_idcs_set] = new_idx\n",
    "            idx_to_chord[new_idx] = note_idcs_set\n",
    "        else:\n",
    "            note_set_idx = chord_to_idx[note_idcs_set]\n",
    "    return [chord_to_idx, idx_to_chord]\n",
    "\n",
    "results = encode_midi_states(daft_state)\n",
    "print (len(results[0]))\n",
    "print (len(results[1]))\n",
    "chord_to_idx = results[0]\n",
    "idx_to_chord = results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "randnum = random.sample(range(len(results[0])),1)[0]\n",
    "print randnum\n",
    "assert(results[0][results[1][randnum]] == randnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2402"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now the set of chords is a library of possible classes\n",
    "# return a num_notes x len_chord_set\n",
    "def encode_song(midi_states, chord_to_idx):\n",
    "    song_encoded = np.zeros((len(midi_states)+1, len(chord_to_idx)+1))\n",
    "    for index in range(len(midi_states)):\n",
    "        notes_only = [note[0] for note in midi_states[index]]\n",
    "        note_idcs_set = tuple([i for i, x in enumerate(notes_only) if x == 1])\n",
    "        if len(note_idcs_set) == 0: note_idcs_set = (None)\n",
    "        library_idx = chord_to_idx[note_idcs_set]\n",
    "        song_encoded.itemset((index,library_idx), 1)\n",
    "    return song_encoded\n",
    "\n",
    "encoded_song = encode_song(daft_state, results[0])\n",
    "len(encoded_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n"
     ]
    }
   ],
   "source": [
    "randnum = random.sample(range(len(encoded_song)),1)[0]\n",
    "print randnum\n",
    "assert(sum(encoded_song[randnum]) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(788, 40, 382)\n",
      "(788, 382)\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "chord_to_idx = results[0]\n",
    "idx_to_chord = results[1]\n",
    "\n",
    "maxlen = 40\n",
    "step = 3\n",
    "chunks = []\n",
    "next_chords = []\n",
    "for i in range(0, len(encoded_song) - maxlen, step):\n",
    "    chunks.append(encoded_song[i: i + maxlen])\n",
    "    next_chords.append(encoded_song[i + maxlen])\n",
    "\n",
    "# len(chunks) should == len(daft_state)/step\n",
    "assert(len(chunks) == len(next_chords))\n",
    "\n",
    "X = np.array(chunks)\n",
    "y = np.array(next_chords)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(sum(y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GRID K520 (CNMeM is disabled, cuDNN Version is too old. Update to v5, was 2000.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# build the model: 2 stacked LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, len(chord_to_idx)+1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chord_to_idx)+1))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "788/788 [==============================] - 2s - loss: 5.6790 - acc: 0.0711     \n",
      "Epoch 2/100\n",
      "788/788 [==============================] - 2s - loss: 4.9951 - acc: 0.0749     \n",
      "Epoch 3/100\n",
      "788/788 [==============================] - 2s - loss: 4.7024 - acc: 0.0812     \n",
      "Epoch 4/100\n",
      "788/788 [==============================] - 2s - loss: 4.7192 - acc: 0.0622     \n",
      "Epoch 5/100\n",
      "788/788 [==============================] - 2s - loss: 4.7754 - acc: 0.0761     \n",
      "Epoch 6/100\n",
      "788/788 [==============================] - 2s - loss: 4.7486 - acc: 0.0520     \n",
      "Epoch 7/100\n",
      "788/788 [==============================] - 2s - loss: 4.6055 - acc: 0.0863     \n",
      "Epoch 8/100\n",
      "788/788 [==============================] - 2s - loss: 4.5032 - acc: 0.0838     \n",
      "Epoch 9/100\n",
      "788/788 [==============================] - 2s - loss: 4.4756 - acc: 0.0863     \n",
      "Epoch 10/100\n",
      "788/788 [==============================] - 2s - loss: 4.4689 - acc: 0.0863     \n",
      "Epoch 11/100\n",
      "788/788 [==============================] - 2s - loss: 4.4467 - acc: 0.0749     \n",
      "Epoch 12/100\n",
      "788/788 [==============================] - 2s - loss: 4.3114 - acc: 0.0952     \n",
      "Epoch 13/100\n",
      "788/788 [==============================] - 2s - loss: 4.1916 - acc: 0.1079     \n",
      "Epoch 14/100\n",
      "788/788 [==============================] - 2s - loss: 4.1455 - acc: 0.1180     \n",
      "Epoch 15/100\n",
      "788/788 [==============================] - 2s - loss: 4.1561 - acc: 0.1079     \n",
      "Epoch 16/100\n",
      "788/788 [==============================] - 2s - loss: 4.3612 - acc: 0.1180     \n",
      "Epoch 17/100\n",
      "788/788 [==============================] - 2s - loss: 4.2776 - acc: 0.1028     \n",
      "Epoch 18/100\n",
      "788/788 [==============================] - 2s - loss: 4.0362 - acc: 0.1206     \n",
      "Epoch 19/100\n",
      "788/788 [==============================] - 2s - loss: 4.0738 - acc: 0.1307     \n",
      "Epoch 20/100\n",
      "788/788 [==============================] - 2s - loss: 4.1226 - acc: 0.1117     \n",
      "Epoch 21/100\n",
      "788/788 [==============================] - 2s - loss: 3.9701 - acc: 0.1371     \n",
      "Epoch 22/100\n",
      "788/788 [==============================] - 2s - loss: 4.0768 - acc: 0.1269     \n",
      "Epoch 23/100\n",
      "788/788 [==============================] - 2s - loss: 3.9920 - acc: 0.1218     \n",
      "Epoch 24/100\n",
      "788/788 [==============================] - 2s - loss: 3.9563 - acc: 0.1396     \n",
      "Epoch 25/100\n",
      "788/788 [==============================] - 2s - loss: 4.0010 - acc: 0.1244     \n",
      "Epoch 26/100\n",
      "788/788 [==============================] - 2s - loss: 4.0554 - acc: 0.1396     \n",
      "Epoch 27/100\n",
      "788/788 [==============================] - 2s - loss: 3.9910 - acc: 0.1396     \n",
      "Epoch 28/100\n",
      "788/788 [==============================] - 2s - loss: 4.4581 - acc: 0.1282     \n",
      "Epoch 29/100\n",
      "788/788 [==============================] - 2s - loss: 3.9474 - acc: 0.1459     \n",
      "Epoch 30/100\n",
      "788/788 [==============================] - 2s - loss: 3.8473 - acc: 0.1662     \n",
      "Epoch 31/100\n",
      "788/788 [==============================] - 2s - loss: 3.8921 - acc: 0.1485     \n",
      "Epoch 32/100\n",
      "788/788 [==============================] - 2s - loss: 3.8485 - acc: 0.1447     \n",
      "Epoch 33/100\n",
      "788/788 [==============================] - 2s - loss: 3.8403 - acc: 0.1574     \n",
      "Epoch 34/100\n",
      "788/788 [==============================] - 2s - loss: 3.7956 - acc: 0.1396     \n",
      "Epoch 35/100\n",
      "788/788 [==============================] - 2s - loss: 3.7556 - acc: 0.1536     \n",
      "Epoch 36/100\n",
      "788/788 [==============================] - 2s - loss: 3.6118 - acc: 0.1739     \n",
      "Epoch 37/100\n",
      "788/788 [==============================] - 2s - loss: 3.6863 - acc: 0.1637     \n",
      "Epoch 38/100\n",
      "788/788 [==============================] - 2s - loss: 3.7841 - acc: 0.1612     \n",
      "Epoch 39/100\n",
      "788/788 [==============================] - 2s - loss: 4.0048 - acc: 0.1624     \n",
      "Epoch 40/100\n",
      "788/788 [==============================] - 2s - loss: 3.7123 - acc: 0.1675     \n",
      "Epoch 41/100\n",
      "788/788 [==============================] - 2s - loss: 3.5556 - acc: 0.1891     \n",
      "Epoch 42/100\n",
      "788/788 [==============================] - 2s - loss: 3.5515 - acc: 0.1789     \n",
      "Epoch 43/100\n",
      "788/788 [==============================] - 2s - loss: 3.7041 - acc: 0.1891     \n",
      "Epoch 44/100\n",
      "788/788 [==============================] - 2s - loss: 3.5424 - acc: 0.1789     \n",
      "Epoch 45/100\n",
      "788/788 [==============================] - 2s - loss: 3.4659 - acc: 0.1992     \n",
      "Epoch 46/100\n",
      "788/788 [==============================] - 2s - loss: 3.3900 - acc: 0.2018     \n",
      "Epoch 47/100\n",
      "788/788 [==============================] - 2s - loss: 3.3120 - acc: 0.2183     \n",
      "Epoch 48/100\n",
      "788/788 [==============================] - 2s - loss: 3.2836 - acc: 0.2195     \n",
      "Epoch 49/100\n",
      "788/788 [==============================] - 2s - loss: 3.2270 - acc: 0.2475     \n",
      "Epoch 50/100\n",
      "788/788 [==============================] - 2s - loss: 3.2780 - acc: 0.2208     \n",
      "Epoch 51/100\n",
      "788/788 [==============================] - 2s - loss: 3.1387 - acc: 0.2297     \n",
      "Epoch 52/100\n",
      "788/788 [==============================] - 2s - loss: 3.0912 - acc: 0.2398     \n",
      "Epoch 53/100\n",
      "788/788 [==============================] - 2s - loss: 3.1686 - acc: 0.2208     \n",
      "Epoch 54/100\n",
      "788/788 [==============================] - 2s - loss: 3.1692 - acc: 0.2348     \n",
      "Epoch 55/100\n",
      "788/788 [==============================] - 2s - loss: 3.2672 - acc: 0.2221     \n",
      "Epoch 56/100\n",
      "788/788 [==============================] - 2s - loss: 3.0764 - acc: 0.2538     \n",
      "Epoch 57/100\n",
      "788/788 [==============================] - 2s - loss: 2.9432 - acc: 0.2817     \n",
      "Epoch 58/100\n",
      "788/788 [==============================] - 2s - loss: 2.9509 - acc: 0.2424     \n",
      "Epoch 59/100\n",
      "788/788 [==============================] - 2s - loss: 2.8802 - acc: 0.2652     \n",
      "Epoch 60/100\n",
      "788/788 [==============================] - 2s - loss: 2.8913 - acc: 0.2805     \n",
      "Epoch 61/100\n",
      "788/788 [==============================] - 2s - loss: 3.0397 - acc: 0.2272     \n",
      "Epoch 62/100\n",
      "788/788 [==============================] - 2s - loss: 2.9119 - acc: 0.2589     \n",
      "Epoch 63/100\n",
      "788/788 [==============================] - 2s - loss: 2.8703 - acc: 0.2538     \n",
      "Epoch 64/100\n",
      "788/788 [==============================] - 2s - loss: 2.8735 - acc: 0.2678     \n",
      "Epoch 65/100\n",
      "788/788 [==============================] - 2s - loss: 3.2308 - acc: 0.2437     \n",
      "Epoch 66/100\n",
      "788/788 [==============================] - 2s - loss: 3.0822 - acc: 0.2475     \n",
      "Epoch 67/100\n",
      "788/788 [==============================] - 2s - loss: 2.8465 - acc: 0.2843     \n",
      "Epoch 68/100\n",
      "788/788 [==============================] - 2s - loss: 2.8888 - acc: 0.2652     \n",
      "Epoch 69/100\n",
      "788/788 [==============================] - 2s - loss: 2.8423 - acc: 0.2665     \n",
      "Epoch 70/100\n",
      "788/788 [==============================] - 2s - loss: 2.8605 - acc: 0.2627     \n",
      "Epoch 71/100\n",
      "788/788 [==============================] - 2s - loss: 2.7171 - acc: 0.2665     \n",
      "Epoch 72/100\n",
      "788/788 [==============================] - 2s - loss: 2.6864 - acc: 0.3096     \n",
      "Epoch 73/100\n",
      "788/788 [==============================] - 2s - loss: 2.6362 - acc: 0.2931     \n",
      "Epoch 74/100\n",
      "788/788 [==============================] - 2s - loss: 2.5154 - acc: 0.3122     \n",
      "Epoch 75/100\n",
      "788/788 [==============================] - 2s - loss: 2.5430 - acc: 0.3211     \n",
      "Epoch 76/100\n",
      "788/788 [==============================] - 2s - loss: 2.4791 - acc: 0.3198     \n",
      "Epoch 77/100\n",
      "788/788 [==============================] - 2s - loss: 2.5116 - acc: 0.3173     \n",
      "Epoch 78/100\n",
      "788/788 [==============================] - 2s - loss: 2.5202 - acc: 0.3084     \n",
      "Epoch 79/100\n",
      "788/788 [==============================] - 2s - loss: 2.3579 - acc: 0.3579     \n",
      "Epoch 80/100\n",
      "788/788 [==============================] - 2s - loss: 2.3890 - acc: 0.3414     \n",
      "Epoch 81/100\n",
      "788/788 [==============================] - 2s - loss: 2.2664 - acc: 0.3655     \n",
      "Epoch 82/100\n",
      "788/788 [==============================] - 2s - loss: 2.3080 - acc: 0.3388     \n",
      "Epoch 83/100\n",
      "788/788 [==============================] - 2s - loss: 2.3559 - acc: 0.3350     \n",
      "Epoch 84/100\n",
      "788/788 [==============================] - 2s - loss: 2.2403 - acc: 0.3553     \n",
      "Epoch 85/100\n",
      "788/788 [==============================] - 2s - loss: 2.2566 - acc: 0.3591     \n",
      "Epoch 86/100\n",
      "788/788 [==============================] - 2s - loss: 2.4107 - acc: 0.3388     \n",
      "Epoch 87/100\n",
      "788/788 [==============================] - 2s - loss: 2.3165 - acc: 0.3756     \n",
      "Epoch 88/100\n",
      "788/788 [==============================] - 2s - loss: 2.1636 - acc: 0.3832     \n",
      "Epoch 89/100\n",
      "788/788 [==============================] - 2s - loss: 2.0881 - acc: 0.3756     \n",
      "Epoch 90/100\n",
      "788/788 [==============================] - 2s - loss: 1.9793 - acc: 0.4188     \n",
      "Epoch 91/100\n",
      "788/788 [==============================] - 2s - loss: 1.9894 - acc: 0.4150     \n",
      "Epoch 92/100\n",
      "788/788 [==============================] - 2s - loss: 2.0261 - acc: 0.3947     \n",
      "Epoch 93/100\n",
      "788/788 [==============================] - 2s - loss: 2.0049 - acc: 0.3871     \n",
      "Epoch 94/100\n",
      "788/788 [==============================] - 2s - loss: 1.8441 - acc: 0.4594     \n",
      "Epoch 95/100\n",
      "788/788 [==============================] - 2s - loss: 1.8716 - acc: 0.4404     \n",
      "Epoch 96/100\n",
      "788/788 [==============================] - 2s - loss: 1.8096 - acc: 0.4530     \n",
      "Epoch 97/100\n",
      "788/788 [==============================] - 2s - loss: 1.8003 - acc: 0.4721     \n",
      "Epoch 98/100\n",
      "788/788 [==============================] - 2s - loss: 1.8469 - acc: 0.4454     \n",
      "Epoch 99/100\n",
      "788/788 [==============================] - 2s - loss: 1.8009 - acc: 0.4378     \n",
      "Epoch 100/100\n",
      "788/788 [==============================] - 2s - loss: 1.7254 - acc: 0.4835     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f00f8069510>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=128, nb_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(a, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    a = np.log(a) / temperature\n",
    "    a = np.exp(a) / np.sum(np.exp(a))\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))\n",
    "\n",
    "start_index = random.randint(0, len(encoded_song) - maxlen - 1)\n",
    "\n",
    "for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "    print()\n",
    "    print('----- diversity:', diversity)\n",
    "\n",
    "    generated = []\n",
    "    song_chunk = encoded_song[start_index: start_index + maxlen]\n",
    "    generated.append(song_chunk)\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(400):\n",
    "        x = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
