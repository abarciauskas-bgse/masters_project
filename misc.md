# Useful ways to think about nnets

> ConvNets can be interpreted as gradually transforming the images into a representation in which the classes are separable by a linear classifier.

[Source](http://cs231n.github.io/understanding-cnn/)


> No way to hard-code a cat

(Class notes cs231n Lecture 2 on kNN and Linear Classifiers)

> There is a semantic gap between images and their translation into features which are interpretable by computers. Common challenges to linear classification: viewpoint, illumination, deformation, occlusion, background noise, and intraclass objects.

(Class notes cs231n Lecture 2 on kNN and Linear Classifiers)

# Curiousities

> Large modern neural networks are even harder to study because of their size; for example, understanding the widely-used AlexNet DNN involves making sense of the values taken by the 60 mil- lion trained network parameters.

